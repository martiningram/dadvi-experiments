{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_and_tidy_lib import GetMetdataDataframe, GetMethodDataframe\n",
    "from load_results_lib import VALID_METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = '../blade_runs/'\n",
    "\n",
    "folder_method_list = (\n",
    "    (join(base_folder, \"nuts_results/\"), 'NUTS'),\n",
    "    (join(base_folder, \"dadvi_results/\"), 'DADVI'),\n",
    "    (join(base_folder, \"lrvb_results/\"), 'LRVB'),\n",
    "    (join(base_folder, \"raabbvi_results/\"), 'RAABBVI'),\n",
    "    (join(base_folder, \"sadvi_results/\"), 'SADVI'),\n",
    "    (join(base_folder, \"sfullrank_advi_results/\"), 'SADVI_FR'),\n",
    "    (join(base_folder, 'lrvb_doubling_results'), 'LRVB_Doubling')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that VALID_METHODS and the loaded methods are the same\n",
    "assert(len(set(all_results.keys()).symmetric_difference(VALID_METHODS)) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which models are missing for which method.  If a model / method pair is missing we\n",
    "# should know why!\n",
    "\n",
    "method_models = {x: all_results[x]['model_name'].tolist() for x in VALID_METHODS}\n",
    "all_models = set().union(*[ v for k, v in method_models.items() ])\n",
    "all_missing_models = set([])\n",
    "\n",
    "for method in VALID_METHODS:\n",
    "    print(method)\n",
    "    missing_models = all_models.difference(method_models[method])\n",
    "    if len(missing_models) > 0:\n",
    "        print('Missing models:')\n",
    "        print('\\n'.join(missing_models))\n",
    "        all_missing_models = all_missing_models.union(missing_models)\n",
    "    else:\n",
    "        print('Nothing missing!')\n",
    "    print('\\n')\n",
    "\n",
    "print('Missing for at least one method: ', all_missing_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove models that are missing for at least one method, in order\n",
    "# to avoid biasing results.\n",
    "for method in VALID_METHODS:\n",
    "    is_missing = all_results[method]['model_name'].apply(lambda x: x in all_missing_models)\n",
    "    print(f'Removing {np.sum(is_missing)} from {method}')\n",
    "    all_results[method] = all_results[method][np.logical_not(is_missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raabbvi_maxiter = 19900\n",
    "\n",
    "method_1 = 'LRVB_Doubling'\n",
    "method_2 = 'RAABBVI'\n",
    "\n",
    "#method_1_df = add_deviation_stats(all_results[method_1], all_results['NUTS']).dropna()\n",
    "method_1_df = add_deviation_stats(all_results[method_1], all_results['NUTS'])\n",
    "\n",
    "#method_2_df = add_deviation_stats(all_results[method_2], all_results['NUTS']).dropna()\n",
    "method_2_df = add_deviation_stats(all_results[method_2], all_results['NUTS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_1_df = add_derived_stats(method_1_df)\n",
    "method_2_df = add_derived_stats(method_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_1_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(method_1_df['mean_deviations_flat'][1])\n",
    "print(method_1_df['mean_deviations'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data saved for each method.  Not all metadata is the same.\n",
    "for method in VALID_METHODS:\n",
    "    print(f'{method}: {all_results[method].keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the proportion of modles that converged for each method\n",
    "for method in VALID_METHODS:\n",
    "    prop_converged = all_results[method].get('converged', None)\n",
    "    if prop_converged is not None:\n",
    "        print(f'Proportion converged for {method:10}: {prop_converged.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = method_1_df.merge(\n",
    "    method_2_df, on='model_name',\n",
    "    suffixes=(f'_{method_1}', f'_{method_2}'))\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "xmin, xmax = [comparison[f'mean_rms_{method_1}'].min(), comparison[f'mean_rms_{method_1}'].max()]\n",
    "# ax.scatter(comparison['mean_rms_raabbvi'], comparison['mean_rms_lrvb'], c=comparison['converged'])\n",
    "ax.scatter(comparison[f'mean_rms_{method_1}'], comparison[f'mean_rms_{method_2}'])\n",
    "ax.plot([xmin, xmax], [xmin, xmax])\n",
    "\n",
    "for row in comparison.itertuples():\n",
    "    ax.annotate(row.model_name, (getattr(row, f'mean_rms_{method_1}'), getattr(row, f'mean_rms_{method_2}')))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(f'RMSE mean scaled by posterior sd, {method_1}')\n",
    "ax.set_ylabel(f'RMSE mean scaled by posterior sd, {method_2}')\n",
    "\n",
    "ax.grid(alpha=0.5, linestyle='--')\n",
    "\n",
    "f.set_size_inches(12, 8)\n",
    "f.tight_layout()\n",
    "\n",
    "# plt.savefig('./mean_comparison.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "xmin, xmax = [comparison[f'sd_rms_{method_1}'].min(), comparison[f'sd_rms_{method_1}'].max()]\n",
    "# ax.scatter(comparison['mean_rms_raabbvi'], comparison['mean_rms_lrvb'], c=comparison['converged'])\n",
    "ax.scatter(comparison[f'sd_rms_{method_1}'], comparison[f'sd_rms_{method_2}'])\n",
    "ax.plot([xmin, xmax], [xmin, xmax])\n",
    "\n",
    "for row in comparison.itertuples():\n",
    "    ax.annotate(row.model_name, (getattr(row, f'sd_rms_{method_1}'), getattr(row, f'sd_rms_{method_2}')))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(f'RMSE sd scaled by posterior sd, {method_1}')\n",
    "ax.set_ylabel(f'RMSE sd scaled by posterior sd, {method_2}')\n",
    "\n",
    "ax.grid(alpha=0.5, linestyle='--')\n",
    "\n",
    "f.set_size_inches(12, 8)\n",
    "f.tight_layout()\n",
    "\n",
    "# plt.savefig('./sd_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f, ax = plt.subplots(1, 1)\n",
    "\n",
    "xmin, xmax = [comparison[f'runtime_{method_1}'].min(), comparison[f'runtime_{method_1}'].max()]\n",
    "# ax.scatter(comparison['mean_rms_raabbvi'], comparison['mean_rms_lrvb'], c=comparison['converged'])\n",
    "ax.scatter(comparison[f'runtime_{method_1}'], comparison[f'runtime_{method_2}'])\n",
    "ax.plot([xmin, xmax], [xmin, xmax])\n",
    "\n",
    "for row in comparison.itertuples():\n",
    "    ax.annotate(row.model_name, (getattr(row, f'runtime_{method_1}'), getattr(row, f'runtime_{method_2}')))\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax.set_xlabel(f'Runtime, {method_1}')\n",
    "ax.set_ylabel(f'Runtime, {method_2}')\n",
    "\n",
    "ax.grid(alpha=0.5, linestyle='--')\n",
    "\n",
    "f.set_size_inches(12, 8)\n",
    "f.tight_layout()\n",
    "\n",
    "# plt.savefig('runtime_comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['LRVB_Doubling']['M'] = all_results['LRVB_Doubling']['metadata'].apply(lambda x: x['M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['LRVB_Doubling'][['model_name', 'runtime', 'M']].sort_values('M', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results['LRVB'][['model_name', 'runtime']].sort_values('runtime')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dadvi-experiments",
   "language": "python",
   "name": "dadvi-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6b81382af3beaebbac93aac606334308960b1a7270e498d25ccdd66c34d7f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
