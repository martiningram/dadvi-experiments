Ryan, I think you mentioned in our last meeting that you'd like to start taking
a look at the experimental results, right? There are basically three parts to
this: Comparison of moments (means, sds) and runtime, Coverage analysis for ARM &
non-ARM models, except POTUS Coverage analysis for POTUS

For (1), code is here: comparison/analysis/compare_moment_estimates.ipynb
and data is here:
https://drive.google.com/file/d/1y-k5unAPPAjI3kPrzpJAwGcfA7Dmr8Dx/view?usp=share_link
unzip model_runs.zip -d comparison/blade_runs/

For (2), code is here:
comparison/analysis/DADVI%20coverage.ipynb and the data is here:
https://drive.google.com/file/d/1Ey0NEvEGbLowePTwvDjwHxehxLbqh7MB/view?usp=share_link

I can also send (3) through, but I'm still having some convergence issues with
CG. I thought a preconditioner might help, but it seems to have made things
worse! Maybe I'm computing it wrongly? Code is here if you'd like to have a
look, Ryan:

https://github.com/martiningram/dadvi-experiments/blob/add-coverage-reruns-without-cov-computations/comparison/analysis/compute_potus_freq_sds.py#L70 .

For (1), I should rerun things and save the final Newton step norm to make sure
DADVI always converges. But I thought I'd send this through already, so that you
can have a first look. Let me know if you have any questions :)


----------------
Notes for Martin

- Let's have some kind of version control for results to make sure
we're looking at the right date.  Probably a logfile with the stdout
and a datetime stamp would be good, assuming that all the models for a
particular method are run at the same time.
- Every method should have a 'converged' metadata tag using its own appropriate
metric.
-
