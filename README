Ryan, I think you mentioned in our last meeting that you'd like to start taking
a look at the experimental results, right? There are basically three parts to
this: Comparison of moments (means, sds) and runtime, Coverage analysis for ARM &
non-ARM models, except POTUS Coverage analysis for POTUS

For (1), code is here: comparison/analysis/compare_moment_estimates.ipynb
and data is here:
https://drive.google.com/file/d/1y-k5unAPPAjI3kPrzpJAwGcfA7Dmr8Dx/view?usp=share_link
unzip model_runs.zip -d comparison/blade_runs/

For (2), code is here:
comparison/analysis/DADVI%20coverage.ipynb and the data is here:
https://drive.google.com/file/d/1Ey0NEvEGbLowePTwvDjwHxehxLbqh7MB/view?usp=share_link

I can also send (3) through, but I'm still having some convergence issues with
CG.

For (1), I should rerun things and save the final Newton step norm to make sure
DADVI always converges. But I thought I'd send this through already, so that you
can have a first look. Let me know if you have any questions :)


----------------
Notes for Martin


- Every method should have a 'converged' metadata tag using its own appropriate
metric.  I agree with your inlinen TODO, let's make sure rhat is in there for NUTS
- I see the metadata doesn't really contain evaluation counts for all models,
and is using runtime now.  Are evaluation counts saved somewhere else?
- The last two indices of beta_full for the NUTS microcredit model are mean
zero and zero standard deviation.  Any idea what's going on there?
- It would be nice to have some kind of version control for results to make sure
we're looking at the right date.  I think maybe we could save a timestamp
and maybe machine id in all the metadata.  What do you think?


----------------
Installation notes

I had a very hard time getting RAABBVI to work with recent python versions. Ultimately, I reverted back to older stuff to get that part to run. The env is called `functioning_raabbvi_env.yaml`.
